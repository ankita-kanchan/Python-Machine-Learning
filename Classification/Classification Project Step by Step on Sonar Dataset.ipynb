{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Poject Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Model_accuracy\n",
    "#2. Best_model\n",
    "#3.Best_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold,GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare multiple models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\"LogisticRegression\":LogisticRegression(),\"DecisionTreeClassifier\":DecisionTreeClassifier(),\"SVM\":SVC()\n",
    "        ,\"KNN\":KNeighborsClassifier(),\"GNB\":GaussianNB(),\"RandomForestClassifier\":RandomForestClassifier(),\n",
    "       \"AdaBoostClassifier\":AdaBoostClassifier(),\"GradientBoostingClassifier\":GradientBoostingClassifier(),\n",
    "       \"XGBClassifier\":XGBClassifier()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for training the multiple models and generating accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelAccuracy-> models,x,y,scaleFlag=0,1,2\n",
    "def modelAccuracy(models,x,y,scaleFlag):\n",
    "    xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=0)\n",
    "    acc_result={}\n",
    "    for name,model in models.items():\n",
    "        if(scaleFlag==1):\n",
    "            model_pipeline=Pipeline([(\"MinMax\",MinMaxScaler()),('model',model)])\n",
    "        elif(scaleFlag==2):\n",
    "            model_pipeline=Pipeline([(\"standardScaler\",StandardScaler()),('model',model)])\n",
    "        else:\n",
    "            model_pipeline=Pipeline([('model',model)])\n",
    "        #model train and prediction\n",
    "        model_fit=model_pipeline.fit(xtrain,ytrain)\n",
    "        ypred=model_fit.predict(xtest)\n",
    "        acc=accuracy_score(ytest,ypred)\n",
    "        print(\"The Accuracy for \",name,\" is :\",acc)\n",
    "        acc_result[name]=acc\n",
    "    return acc_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for getting a model with highest accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestModel(model_result):\n",
    "    high=0\n",
    "    for name,acc in model_result.items():\n",
    "        if acc>high:\n",
    "            high=acc\n",
    "            model_name=name\n",
    "    print(\"Best model is \",model_name,\" with Accuracy \",high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for getting a best models best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestParameter(model,params,x,y):\n",
    "    cv=RepeatedStratifiedKFold(n_splits=10,n_repeats=3)\n",
    "    grid_cv=GridSearchCV(estimator=model,param_grid=params,cv=cv,scoring=\"accuracy\")\n",
    "    res=grid_cv.fit(x,y)\n",
    "    print(\"Best Parameters are \",res.best_params_)\n",
    "    print(\"Best Accuracy is \",res.best_score_)\n",
    "    Model_cv=pd.DataFrame(res.cv_results_ )\n",
    "    print(Model_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Data to find out suitable model for this classification dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../Data/sonar.all-data.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "56    0\n",
       "57    0\n",
       "58    0\n",
       "59    0\n",
       "60    0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "df[60]=le.fit_transform(df[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(columns=[60])\n",
    "y=df[60]\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy for  LogisticRegression  is : 0.8333333333333334\n",
      "The Accuracy for  DecisionTreeClassifier  is : 0.6904761904761905\n",
      "The Accuracy for  SVM  is : 0.7857142857142857\n",
      "The Accuracy for  KNN  is : 0.7857142857142857\n",
      "The Accuracy for  GNB  is : 0.6666666666666666\n",
      "The Accuracy for  RandomForestClassifier  is : 0.8571428571428571\n",
      "The Accuracy for  AdaBoostClassifier  is : 0.8809523809523809\n",
      "The Accuracy for  GradientBoostingClassifier  is : 0.9047619047619048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:16:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "The Accuracy for  XGBClassifier  is : 0.8809523809523809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "acc=modelAccuracy(models,x,y,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': 0.8333333333333334,\n",
       " 'DecisionTreeClassifier': 0.6904761904761905,\n",
       " 'SVM': 0.7857142857142857,\n",
       " 'KNN': 0.7857142857142857,\n",
       " 'GNB': 0.6666666666666666,\n",
       " 'RandomForestClassifier': 0.8571428571428571,\n",
       " 'AdaBoostClassifier': 0.8809523809523809,\n",
       " 'GradientBoostingClassifier': 0.9047619047619048,\n",
       " 'XGBClassifier': 0.8809523809523809}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is  GradientBoostingClassifier with Accuracy  0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "bestModel(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters are  {'learning_rate': 0.3, 'loss': 'exponential', 'n_estimators': 100}\n",
      "Best Accuracy is  0.818014705882353\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.027357      0.003805         0.002627        0.000870   \n",
      "1        0.186516      0.023962         0.003887        0.001489   \n",
      "2        0.330767      0.012130         0.003690        0.001326   \n",
      "3        0.035723      0.002325         0.003045        0.000920   \n",
      "4        0.164990      0.005729         0.003326        0.000979   \n",
      "5        0.332115      0.013517         0.003524        0.000965   \n",
      "6        0.035284      0.001526         0.003090        0.000904   \n",
      "7        0.182652      0.012793         0.003890        0.001636   \n",
      "8        0.338778      0.030669         0.004017        0.001756   \n",
      "9        0.037430      0.002912         0.003522        0.001650   \n",
      "10       0.165551      0.006307         0.003243        0.001034   \n",
      "11       0.320951      0.010333         0.003488        0.001053   \n",
      "12       0.036729      0.003347         0.003356        0.001555   \n",
      "13       0.158255      0.006392         0.003692        0.001768   \n",
      "14       0.323686      0.015151         0.003704        0.001621   \n",
      "15       0.034441      0.001334         0.003188        0.001045   \n",
      "16       0.164797      0.007605         0.003319        0.001043   \n",
      "17       0.321414      0.011542         0.003545        0.001216   \n",
      "18       0.033713      0.001899         0.003209        0.000971   \n",
      "19       0.160965      0.005611         0.003864        0.001455   \n",
      "20       0.343050      0.030486         0.003556        0.001450   \n",
      "21       0.035677      0.002869         0.003496        0.001476   \n",
      "22       0.170030      0.013341         0.003656        0.001375   \n",
      "23       0.321039      0.012885         0.003268        0.001127   \n",
      "24       0.035200      0.003375         0.003130        0.001028   \n",
      "25       0.158498      0.005504         0.003158        0.000734   \n",
      "26       0.328917      0.019476         0.003553        0.001226   \n",
      "27       0.046503      0.003453         0.004290        0.001934   \n",
      "28       0.185076      0.017728         0.004246        0.001714   \n",
      "29       0.336389      0.019211         0.003113        0.000756   \n",
      "30       0.036019      0.002527         0.003134        0.001119   \n",
      "31       0.169126      0.012001         0.003455        0.001331   \n",
      "32       0.324804      0.009941         0.003486        0.001175   \n",
      "33       0.036156      0.002629         0.003293        0.001130   \n",
      "34       0.166588      0.008094         0.003061        0.001071   \n",
      "35       0.329806      0.010742         0.003123        0.001015   \n",
      "\n",
      "   param_learning_rate   param_loss param_n_estimators  \\\n",
      "0                  0.3     deviance                 10   \n",
      "1                  0.3     deviance                 50   \n",
      "2                  0.3     deviance                100   \n",
      "3                  0.3  exponential                 10   \n",
      "4                  0.3  exponential                 50   \n",
      "5                  0.3  exponential                100   \n",
      "6                  0.1     deviance                 10   \n",
      "7                  0.1     deviance                 50   \n",
      "8                  0.1     deviance                100   \n",
      "9                  0.1  exponential                 10   \n",
      "10                 0.1  exponential                 50   \n",
      "11                 0.1  exponential                100   \n",
      "12                 0.5     deviance                 10   \n",
      "13                 0.5     deviance                 50   \n",
      "14                 0.5     deviance                100   \n",
      "15                 0.5  exponential                 10   \n",
      "16                 0.5  exponential                 50   \n",
      "17                 0.5  exponential                100   \n",
      "18               0.001     deviance                 10   \n",
      "19               0.001     deviance                 50   \n",
      "20               0.001     deviance                100   \n",
      "21               0.001  exponential                 10   \n",
      "22               0.001  exponential                 50   \n",
      "23               0.001  exponential                100   \n",
      "24                0.01     deviance                 10   \n",
      "25                0.01     deviance                 50   \n",
      "26                0.01     deviance                100   \n",
      "27                0.01  exponential                 10   \n",
      "28                0.01  exponential                 50   \n",
      "29                0.01  exponential                100   \n",
      "30                0.05     deviance                 10   \n",
      "31                0.05     deviance                 50   \n",
      "32                0.05     deviance                100   \n",
      "33                0.05  exponential                 10   \n",
      "34                0.05  exponential                 50   \n",
      "35                0.05  exponential                100   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'learning_rate': 0.3, 'loss': 'deviance', 'n_...           0.823529   \n",
      "1   {'learning_rate': 0.3, 'loss': 'deviance', 'n_...           0.764706   \n",
      "2   {'learning_rate': 0.3, 'loss': 'deviance', 'n_...           0.588235   \n",
      "3   {'learning_rate': 0.3, 'loss': 'exponential', ...           0.529412   \n",
      "4   {'learning_rate': 0.3, 'loss': 'exponential', ...           0.470588   \n",
      "5   {'learning_rate': 0.3, 'loss': 'exponential', ...           0.647059   \n",
      "6   {'learning_rate': 0.1, 'loss': 'deviance', 'n_...           0.588235   \n",
      "7   {'learning_rate': 0.1, 'loss': 'deviance', 'n_...           0.588235   \n",
      "8   {'learning_rate': 0.1, 'loss': 'deviance', 'n_...           0.705882   \n",
      "9   {'learning_rate': 0.1, 'loss': 'exponential', ...           0.588235   \n",
      "10  {'learning_rate': 0.1, 'loss': 'exponential', ...           0.705882   \n",
      "11  {'learning_rate': 0.1, 'loss': 'exponential', ...           0.705882   \n",
      "12  {'learning_rate': 0.5, 'loss': 'deviance', 'n_...           0.647059   \n",
      "13  {'learning_rate': 0.5, 'loss': 'deviance', 'n_...           0.647059   \n",
      "14  {'learning_rate': 0.5, 'loss': 'deviance', 'n_...           0.588235   \n",
      "15  {'learning_rate': 0.5, 'loss': 'exponential', ...           0.529412   \n",
      "16  {'learning_rate': 0.5, 'loss': 'exponential', ...           0.529412   \n",
      "17  {'learning_rate': 0.5, 'loss': 'exponential', ...           0.588235   \n",
      "18  {'learning_rate': 0.001, 'loss': 'deviance', '...           0.529412   \n",
      "19  {'learning_rate': 0.001, 'loss': 'deviance', '...           0.529412   \n",
      "20  {'learning_rate': 0.001, 'loss': 'deviance', '...           0.470588   \n",
      "21  {'learning_rate': 0.001, 'loss': 'exponential'...           0.529412   \n",
      "22  {'learning_rate': 0.001, 'loss': 'exponential'...           0.529412   \n",
      "23  {'learning_rate': 0.001, 'loss': 'exponential'...           0.470588   \n",
      "24  {'learning_rate': 0.01, 'loss': 'deviance', 'n...           0.470588   \n",
      "25  {'learning_rate': 0.01, 'loss': 'deviance', 'n...           0.529412   \n",
      "26  {'learning_rate': 0.01, 'loss': 'deviance', 'n...           0.588235   \n",
      "27  {'learning_rate': 0.01, 'loss': 'exponential',...           0.470588   \n",
      "28  {'learning_rate': 0.01, 'loss': 'exponential',...           0.529412   \n",
      "29  {'learning_rate': 0.01, 'loss': 'exponential',...           0.647059   \n",
      "30  {'learning_rate': 0.05, 'loss': 'deviance', 'n...           0.529412   \n",
      "31  {'learning_rate': 0.05, 'loss': 'deviance', 'n...           0.588235   \n",
      "32  {'learning_rate': 0.05, 'loss': 'deviance', 'n...           0.588235   \n",
      "33  {'learning_rate': 0.05, 'loss': 'exponential',...           0.529412   \n",
      "34  {'learning_rate': 0.05, 'loss': 'exponential',...           0.588235   \n",
      "35  {'learning_rate': 0.05, 'loss': 'exponential',...           0.647059   \n",
      "\n",
      "    split1_test_score  ...  split23_test_score  split24_test_score  \\\n",
      "0            0.882353  ...            0.647059            0.588235   \n",
      "1            0.882353  ...            0.705882            0.705882   \n",
      "2            0.882353  ...            0.823529            0.647059   \n",
      "3            0.823529  ...            0.647059            0.705882   \n",
      "4            0.882353  ...            0.705882            0.705882   \n",
      "5            0.882353  ...            0.823529            0.705882   \n",
      "6            0.764706  ...            0.647059            0.529412   \n",
      "7            0.882353  ...            0.705882            0.529412   \n",
      "8            0.882353  ...            0.764706            0.647059   \n",
      "9            0.764706  ...            0.705882            0.529412   \n",
      "10           0.882353  ...            0.705882            0.588235   \n",
      "11           0.882353  ...            0.823529            0.588235   \n",
      "12           0.882353  ...            0.705882            0.588235   \n",
      "13           0.882353  ...            0.764706            0.705882   \n",
      "14           0.882353  ...            0.764706            0.705882   \n",
      "15           0.764706  ...            0.823529            0.529412   \n",
      "16           0.882353  ...            0.823529            0.529412   \n",
      "17           0.882353  ...            0.823529            0.588235   \n",
      "18           0.529412  ...            0.529412            0.529412   \n",
      "19           0.529412  ...            0.529412            0.529412   \n",
      "20           0.647059  ...            0.529412            0.529412   \n",
      "21           0.529412  ...            0.529412            0.529412   \n",
      "22           0.529412  ...            0.529412            0.529412   \n",
      "23           0.647059  ...            0.529412            0.529412   \n",
      "24           0.647059  ...            0.529412            0.529412   \n",
      "25           0.764706  ...            0.647059            0.529412   \n",
      "26           0.823529  ...            0.705882            0.529412   \n",
      "27           0.647059  ...            0.529412            0.529412   \n",
      "28           0.764706  ...            0.647059            0.529412   \n",
      "29           0.764706  ...            0.705882            0.529412   \n",
      "30           0.823529  ...            0.647059            0.529412   \n",
      "31           0.764706  ...            0.705882            0.529412   \n",
      "32           0.882353  ...            0.705882            0.588235   \n",
      "33           0.764706  ...            0.588235            0.529412   \n",
      "34           0.823529  ...            0.588235            0.588235   \n",
      "35           0.882353  ...            0.588235            0.588235   \n",
      "\n",
      "    split25_test_score  split26_test_score  split27_test_score  \\\n",
      "0             0.882353              0.8125              0.9375   \n",
      "1             0.823529              0.8125              0.9375   \n",
      "2             0.882353              0.8125              0.9375   \n",
      "3             0.823529              0.7500              0.8750   \n",
      "4             0.882353              0.7500              1.0000   \n",
      "5             0.882353              0.7500              1.0000   \n",
      "6             0.823529              0.8750              0.9375   \n",
      "7             0.823529              0.8125              0.9375   \n",
      "8             0.823529              0.8750              0.9375   \n",
      "9             0.764706              0.8125              0.9375   \n",
      "10            0.882353              0.7500              0.9375   \n",
      "11            0.823529              0.8750              0.9375   \n",
      "12            0.823529              0.7500              0.9375   \n",
      "13            0.941176              0.8750              0.9375   \n",
      "14            0.941176              0.8750              0.8750   \n",
      "15            0.764706              0.8750              0.8125   \n",
      "16            0.764706              0.8125              0.8750   \n",
      "17            0.823529              0.8750              0.8125   \n",
      "18            0.529412              0.5625              0.5625   \n",
      "19            0.529412              0.5625              0.5625   \n",
      "20            0.882353              0.6875              0.8750   \n",
      "21            0.529412              0.5625              0.5625   \n",
      "22            0.529412              0.5625              0.5625   \n",
      "23            0.882353              0.6875              0.8750   \n",
      "24            0.882353              0.6250              0.8750   \n",
      "25            0.823529              0.6875              0.9375   \n",
      "26            0.823529              0.8750              0.9375   \n",
      "27            0.882353              0.6875              0.8750   \n",
      "28            0.823529              0.6875              0.9375   \n",
      "29            0.823529              0.8750              0.9375   \n",
      "30            0.823529              0.6875              0.9375   \n",
      "31            0.882353              0.8750              0.9375   \n",
      "32            0.941176              0.8750              0.9375   \n",
      "33            0.764706              0.6875              0.9375   \n",
      "34            0.941176              0.8750              0.9375   \n",
      "35            0.941176              0.8750              0.9375   \n",
      "\n",
      "    split28_test_score  split29_test_score  mean_test_score  std_test_score  \\\n",
      "0               0.7500              0.6250         0.773652        0.115836   \n",
      "1               0.8125              0.6875         0.795711        0.094587   \n",
      "2               0.8125              0.8125         0.817647        0.093692   \n",
      "3               0.9375              0.6875         0.763848        0.098557   \n",
      "4               0.9375              0.8125         0.808456        0.115971   \n",
      "5               0.8750              0.8125         0.818015        0.104469   \n",
      "6               0.7500              0.7500         0.735907        0.099668   \n",
      "7               0.8125              0.6250         0.767892        0.114533   \n",
      "8               0.8125              0.6875         0.809681        0.088254   \n",
      "9               0.7500              0.7500         0.746078        0.094598   \n",
      "10              0.9375              0.7500         0.787868        0.099723   \n",
      "11              0.8750              0.6875         0.801716        0.100253   \n",
      "12              0.8750              0.6875         0.759681        0.092729   \n",
      "13              0.7500              0.7500         0.813480        0.081840   \n",
      "14              0.7500              0.7500         0.807598        0.088227   \n",
      "15              0.9375              0.6875         0.771814        0.104604   \n",
      "16              0.9375              0.8750         0.806495        0.105100   \n",
      "17              0.9375              0.8750         0.816054        0.095661   \n",
      "18              0.5625              0.5625         0.542647        0.016210   \n",
      "19              0.5625              0.5625         0.542647        0.016210   \n",
      "20              0.5625              0.6250         0.647672        0.121428   \n",
      "21              0.5625              0.5625         0.542647        0.016210   \n",
      "22              0.5625              0.5625         0.542647        0.016210   \n",
      "23              0.5625              0.6250         0.635539        0.119085   \n",
      "24              0.6250              0.6250         0.645343        0.109411   \n",
      "25              0.7500              0.7500         0.715931        0.095842   \n",
      "26              0.7500              0.8125         0.752083        0.099933   \n",
      "27              0.5625              0.6250         0.631495        0.116993   \n",
      "28              0.7500              0.7500         0.719853        0.100071   \n",
      "29              0.7500              0.8125         0.757966        0.094884   \n",
      "30              0.7500              0.7500         0.723897        0.107087   \n",
      "31              0.8125              0.7500         0.774510        0.109706   \n",
      "32              0.8750              0.8125         0.787990        0.108205   \n",
      "33              0.7500              0.7500         0.719730        0.100396   \n",
      "34              0.7500              0.7500         0.762377        0.121299   \n",
      "35              0.9375              0.7500         0.790319        0.116467   \n",
      "\n",
      "    rank_test_score  \n",
      "0                15  \n",
      "1                10  \n",
      "2                 2  \n",
      "3                18  \n",
      "4                 6  \n",
      "5                 1  \n",
      "6                24  \n",
      "7                17  \n",
      "8                 5  \n",
      "9                23  \n",
      "10               13  \n",
      "11                9  \n",
      "12               20  \n",
      "13                4  \n",
      "14                7  \n",
      "15               16  \n",
      "16                8  \n",
      "17                3  \n",
      "18               33  \n",
      "19               33  \n",
      "20               29  \n",
      "21               33  \n",
      "22               33  \n",
      "23               31  \n",
      "24               30  \n",
      "25               28  \n",
      "26               22  \n",
      "27               32  \n",
      "28               26  \n",
      "29               21  \n",
      "30               25  \n",
      "31               14  \n",
      "32               12  \n",
      "33               27  \n",
      "34               19  \n",
      "35               11  \n",
      "\n",
      "[36 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "model=GradientBoostingClassifier()\n",
    "params={\"loss\":['deviance', 'exponential'],\"learning_rate\":[0.3,0.1,0.5,0.001,0.01,0.05],\"n_estimators\":[10,50,100]}\n",
    "bestParameter(model,params,xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining The model using best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retraining The model using best parameters\n",
    "model=GradientBoostingClassifier(learning_rate=0.5, loss='deviance',n_estimators= 100)\n",
    "model.fit(xtrain,ytrain)\n",
    "ypred=model.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Best Model on testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.9047619047619048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        21\n",
      "           1       0.90      0.90      0.90        21\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.90      0.90      0.90        42\n",
      "weighted avg       0.90      0.90      0.90        42\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Accuracy is :\",accuracy_score(ytest,ypred))\n",
    "print(classification_report(ytest,ypred))\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "sns.heatmap(cm,annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
